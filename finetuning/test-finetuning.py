import torch
import numpy as np
from transformers import CamembertTokenizer, CamembertModel
import os
from tqdm import tqdm

def test_embedding_pipeline(model_path='data/models/finetuned-camembert', sample_size=100):
    print("ğŸ” DÃ©marrage des tests de vÃ©rification des NaN...")
    
    # 1. Chargement des modÃ¨les
    print("\n1ï¸âƒ£ Chargement des modÃ¨les...")
    try:
        model_name = 'camembert-base'
        tokenizer = CamembertTokenizer.from_pretrained(model_name)
        if os.path.exists(model_path):
            model = CamembertModel.from_pretrained(model_path)
            print("âœ… ModÃ¨le fine-tunÃ© chargÃ© depuis:", model_path)
        else:
            model = CamembertModel.from_pretrained(model_name)
            print("âš ï¸ ModÃ¨le base chargÃ© (pas de fine-tuned trouvÃ©)")
    except Exception as e:
        print(f"âŒ Erreur lors du chargement des modÃ¨les: {str(e)}")
        return
    
    # 2. PrÃ©paration des donnÃ©es de test
    print("\n2ï¸âƒ£ PrÃ©paration des donnÃ©es de test...")
    test_texts = [
        "Un livre trÃ¨s intÃ©ressant",
        "Un trÃ¨s long texte " * 100,  # Test avec un texte long
        "",  # Test avec un texte vide
        "ğŸ˜€ Test avec Ã©mojis ğŸ‰",  # Test avec des caractÃ¨res spÃ©ciaux
        "Test avec des     espaces multiples",
        "Test\navec\ndes\nretours\nÃ \nla\nligne",
    ]
    
    # 3. Tests systÃ©matiques
    print("\n3ï¸âƒ£ ExÃ©cution des tests...")
    results = {
        'total_tested': 0,
        'nan_detected': 0,
        'nan_locations': []
    }
    
    def check_tensor_for_nan(tensor, step_name, text_sample):
        if torch.isnan(tensor).any():
            results['nan_detected'] += 1
            results['nan_locations'].append({
                'step': step_name,
                'text': text_sample[:100] + "..." if len(text_sample) > 100 else text_sample,
                'shape': tensor.shape,
                'nan_count': torch.isnan(tensor).sum().item()
            })
            return True
        return False

    with torch.no_grad():
        for text in tqdm(test_texts, desc="Testing samples"):
            results['total_tested'] += 1
            
            # Test de tokenization
            try:
                inputs = tokenizer(text, 
                                 return_tensors='pt',
                                 truncation=True, 
                                 padding=True, 
                                 max_length=512)
                
                # Test du forward pass
                outputs = model(**inputs)
                
                # VÃ©rification des NaN Ã  chaque Ã©tape
                check_tensor_for_nan(outputs.last_hidden_state, 
                                   "last_hidden_state", text)
                
                # Test de l'embedding CLS
                cls_embedding = outputs.last_hidden_state[:, 0, :]
                check_tensor_for_nan(cls_embedding, "cls_embedding", text)
                
            except Exception as e:
                print(f"\nâŒ Erreur lors du traitement du texte: {text[:100]}...")
                print(f"Error: {str(e)}")
    
    # 4. Rapport final
    print("\n4ï¸âƒ£ Rapport final:")
    print(f"Total des Ã©chantillons testÃ©s: {results['total_tested']}")
    print(f"Nombre de NaN dÃ©tectÃ©s: {results['nan_detected']}")
    
    if results['nan_locations']:
        print("\nDÃ©tails des NaN dÃ©tectÃ©s:")
        for loc in results['nan_locations']:
            print(f"\nğŸ” Ã‰tape: {loc['step']}")
            print(f"ğŸ“ Texte: {loc['text']}")
            print(f"ğŸ“Š Shape: {loc['shape']}")
            print(f"âŒ Nombre de NaN: {loc['nan_count']}")
    else:
        print("\nâœ… Aucun NaN dÃ©tectÃ© !")

if __name__ == "__main__":
    test_embedding_pipeline()
